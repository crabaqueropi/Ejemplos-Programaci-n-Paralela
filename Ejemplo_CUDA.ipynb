{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejemplo CUDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA7Tc_SEJOPo",
        "outputId": "5b8db0bc-5d3c-44f4-a18d-ba90524108dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!echo NVIDIA CUDA AND DRIVES VERIFICATION\n",
        "%cd /usr/local/cuda/samples/1_Utilities/deviceQuery/\n",
        "!ls\n",
        "!make\n",
        "!./deviceQuery\n",
        "!nvcc --version\n",
        "%cat /usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery.cpp"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA CUDA AND DRIVES VERIFICATION\n",
            "/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
            "deviceQuery\t deviceQuery.o\tNsightEclipse.xml  src\n",
            "deviceQuery.cpp  Makefile\treadme.txt\n",
            "make: Nothing to be done for 'all'.\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          10.1 / 10.1\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15080 MBytes (15812263936 bytes)\n",
            "  (40) Multiprocessors, ( 64) CUDA Cores/MP:     2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1\n",
            "Result = PASS\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "/*\n",
            " * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.\n",
            " *\n",
            " * Please refer to the NVIDIA end user license agreement (EULA) associated\n",
            " * with this source code for terms and conditions that govern your use of\n",
            " * this software. Any use, reproduction, disclosure, or distribution of\n",
            " * this software and related documentation outside the terms of the EULA\n",
            " * is strictly prohibited.\n",
            " *\n",
            " */\n",
            "/* This sample queries the properties of the CUDA devices present in the system\n",
            " * via CUDA Runtime API. */\n",
            "\n",
            "// std::system includes\n",
            "\n",
            "#include <cuda_runtime.h>\n",
            "#include <helper_cuda.h>\n",
            "\n",
            "#include <iostream>\n",
            "#include <memory>\n",
            "#include <string>\n",
            "\n",
            "int *pArgc = NULL;\n",
            "char **pArgv = NULL;\n",
            "\n",
            "#if CUDART_VERSION < 5000\n",
            "\n",
            "// CUDA-C includes\n",
            "#include <cuda.h>\n",
            "\n",
            "// This function wraps the CUDA Driver API into a template function\n",
            "template <class T>\n",
            "inline void getCudaAttribute(T *attribute, CUdevice_attribute device_attribute,\n",
            "                             int device) {\n",
            "  CUresult error = cuDeviceGetAttribute(attribute, device_attribute, device);\n",
            "\n",
            "  if (CUDA_SUCCESS != error) {\n",
            "    fprintf(\n",
            "        stderr,\n",
            "        \"cuSafeCallNoSync() Driver API error = %04d from file <%s>, line %i.\\n\",\n",
            "        error, __FILE__, __LINE__);\n",
            "\n",
            "    exit(EXIT_FAILURE);\n",
            "  }\n",
            "}\n",
            "\n",
            "#endif /* CUDART_VERSION < 5000 */\n",
            "\n",
            "////////////////////////////////////////////////////////////////////////////////\n",
            "// Program main\n",
            "////////////////////////////////////////////////////////////////////////////////\n",
            "int main(int argc, char **argv) {\n",
            "  pArgc = &argc;\n",
            "  pArgv = argv;\n",
            "\n",
            "  printf(\"%s Starting...\\n\\n\", argv[0]);\n",
            "  printf(\n",
            "      \" CUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n",
            "\n",
            "  int deviceCount = 0;\n",
            "  cudaError_t error_id = cudaGetDeviceCount(&deviceCount);\n",
            "\n",
            "  if (error_id != cudaSuccess) {\n",
            "    printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\",\n",
            "           static_cast<int>(error_id), cudaGetErrorString(error_id));\n",
            "    printf(\"Result = FAIL\\n\");\n",
            "    exit(EXIT_FAILURE);\n",
            "  }\n",
            "\n",
            "  // This function call returns 0 if there are no CUDA capable devices.\n",
            "  if (deviceCount == 0) {\n",
            "    printf(\"There are no available device(s) that support CUDA\\n\");\n",
            "  } else {\n",
            "    printf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n",
            "  }\n",
            "\n",
            "  int dev, driverVersion = 0, runtimeVersion = 0;\n",
            "\n",
            "  for (dev = 0; dev < deviceCount; ++dev) {\n",
            "    cudaSetDevice(dev);\n",
            "    cudaDeviceProp deviceProp;\n",
            "    cudaGetDeviceProperties(&deviceProp, dev);\n",
            "\n",
            "    printf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
            "\n",
            "    // Console log\n",
            "    cudaDriverGetVersion(&driverVersion);\n",
            "    cudaRuntimeGetVersion(&runtimeVersion);\n",
            "    printf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n",
            "           driverVersion / 1000, (driverVersion % 100) / 10,\n",
            "           runtimeVersion / 1000, (runtimeVersion % 100) / 10);\n",
            "    printf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n",
            "           deviceProp.major, deviceProp.minor);\n",
            "\n",
            "    char msg[256];\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "    sprintf_s(msg, sizeof(msg),\n",
            "             \"  Total amount of global memory:                 %.0f MBytes \"\n",
            "             \"(%llu bytes)\\n\",\n",
            "             static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
            "             (unsigned long long)deviceProp.totalGlobalMem);\n",
            "#else\n",
            "    snprintf(msg, sizeof(msg),\n",
            "             \"  Total amount of global memory:                 %.0f MBytes \"\n",
            "             \"(%llu bytes)\\n\",\n",
            "             static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
            "             (unsigned long long)deviceProp.totalGlobalMem);\n",
            "#endif\n",
            "    printf(\"%s\", msg);\n",
            "\n",
            "    printf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n",
            "           deviceProp.multiProcessorCount,\n",
            "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n",
            "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n",
            "               deviceProp.multiProcessorCount);\n",
            "    printf(\n",
            "        \"  GPU Max Clock rate:                            %.0f MHz (%0.2f \"\n",
            "        \"GHz)\\n\",\n",
            "        deviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n",
            "\n",
            "#if CUDART_VERSION >= 5000\n",
            "    // This is supported in CUDA 5.0 (runtime API device properties)\n",
            "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
            "           deviceProp.memoryClockRate * 1e-3f);\n",
            "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
            "           deviceProp.memoryBusWidth);\n",
            "\n",
            "    if (deviceProp.l2CacheSize) {\n",
            "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
            "             deviceProp.l2CacheSize);\n",
            "    }\n",
            "\n",
            "#else\n",
            "    // This only available in CUDA 4.0-4.2 (but these were only exposed in the\n",
            "    // CUDA Driver API)\n",
            "    int memoryClock;\n",
            "    getCudaAttribute<int>(&memoryClock, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE,\n",
            "                          dev);\n",
            "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
            "           memoryClock * 1e-3f);\n",
            "    int memBusWidth;\n",
            "    getCudaAttribute<int>(&memBusWidth,\n",
            "                          CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, dev);\n",
            "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
            "           memBusWidth);\n",
            "    int L2CacheSize;\n",
            "    getCudaAttribute<int>(&L2CacheSize, CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE, dev);\n",
            "\n",
            "    if (L2CacheSize) {\n",
            "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
            "             L2CacheSize);\n",
            "    }\n",
            "\n",
            "#endif\n",
            "\n",
            "    printf(\n",
            "        \"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, \"\n",
            "        \"%d), 3D=(%d, %d, %d)\\n\",\n",
            "        deviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n",
            "        deviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n",
            "        deviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n",
            "    printf(\n",
            "        \"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n",
            "        deviceProp.maxTexture1DLayered[0], deviceProp.maxTexture1DLayered[1]);\n",
            "    printf(\n",
            "        \"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d \"\n",
            "        \"layers\\n\",\n",
            "        deviceProp.maxTexture2DLayered[0], deviceProp.maxTexture2DLayered[1],\n",
            "        deviceProp.maxTexture2DLayered[2]);\n",
            "\n",
            "    printf(\"  Total amount of constant memory:               %lu bytes\\n\",\n",
            "           deviceProp.totalConstMem);\n",
            "    printf(\"  Total amount of shared memory per block:       %lu bytes\\n\",\n",
            "           deviceProp.sharedMemPerBlock);\n",
            "    printf(\"  Total number of registers available per block: %d\\n\",\n",
            "           deviceProp.regsPerBlock);\n",
            "    printf(\"  Warp size:                                     %d\\n\",\n",
            "           deviceProp.warpSize);\n",
            "    printf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n",
            "           deviceProp.maxThreadsPerMultiProcessor);\n",
            "    printf(\"  Maximum number of threads per block:           %d\\n\",\n",
            "           deviceProp.maxThreadsPerBlock);\n",
            "    printf(\"  Max dimension size of a thread block (x,y,z): (%d, %d, %d)\\n\",\n",
            "           deviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n",
            "           deviceProp.maxThreadsDim[2]);\n",
            "    printf(\"  Max dimension size of a grid size    (x,y,z): (%d, %d, %d)\\n\",\n",
            "           deviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n",
            "           deviceProp.maxGridSize[2]);\n",
            "    printf(\"  Maximum memory pitch:                          %lu bytes\\n\",\n",
            "           deviceProp.memPitch);\n",
            "    printf(\"  Texture alignment:                             %lu bytes\\n\",\n",
            "           deviceProp.textureAlignment);\n",
            "    printf(\n",
            "        \"  Concurrent copy and kernel execution:          %s with %d copy \"\n",
            "        \"engine(s)\\n\",\n",
            "        (deviceProp.deviceOverlap ? \"Yes\" : \"No\"), deviceProp.asyncEngineCount);\n",
            "    printf(\"  Run time limit on kernels:                     %s\\n\",\n",
            "           deviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n",
            "    printf(\"  Integrated GPU sharing Host Memory:            %s\\n\",\n",
            "           deviceProp.integrated ? \"Yes\" : \"No\");\n",
            "    printf(\"  Support host page-locked memory mapping:       %s\\n\",\n",
            "           deviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n",
            "    printf(\"  Alignment requirement for Surfaces:            %s\\n\",\n",
            "           deviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device has ECC support:                        %s\\n\",\n",
            "           deviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "    printf(\"  CUDA Device Driver Mode (TCC or WDDM):         %s\\n\",\n",
            "           deviceProp.tccDriver ? \"TCC (Tesla Compute Cluster Driver)\"\n",
            "                                : \"WDDM (Windows Display Driver Model)\");\n",
            "#endif\n",
            "    printf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n",
            "           deviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device supports Compute Preemption:            %s\\n\",\n",
            "           deviceProp.computePreemptionSupported ? \"Yes\" : \"No\");\n",
            "    printf(\"  Supports Cooperative Kernel Launch:            %s\\n\",\n",
            "           deviceProp.cooperativeLaunch ? \"Yes\" : \"No\");\n",
            "    printf(\"  Supports MultiDevice Co-op Kernel Launch:      %s\\n\",\n",
            "           deviceProp.cooperativeMultiDeviceLaunch ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n",
            "           deviceProp.pciDomainID, deviceProp.pciBusID, deviceProp.pciDeviceID);\n",
            "\n",
            "    const char *sComputeMode[] = {\n",
            "        \"Default (multiple host threads can use ::cudaSetDevice() with device \"\n",
            "        \"simultaneously)\",\n",
            "        \"Exclusive (only one host thread in one process is able to use \"\n",
            "        \"::cudaSetDevice() with this device)\",\n",
            "        \"Prohibited (no host thread can use ::cudaSetDevice() with this \"\n",
            "        \"device)\",\n",
            "        \"Exclusive Process (many threads in one process is able to use \"\n",
            "        \"::cudaSetDevice() with this device)\",\n",
            "        \"Unknown\",\n",
            "        NULL};\n",
            "    printf(\"  Compute Mode:\\n\");\n",
            "    printf(\"     < %s >\\n\", sComputeMode[deviceProp.computeMode]);\n",
            "  }\n",
            "\n",
            "  // If there are 2 or more GPUs, query to determine whether RDMA is supported\n",
            "  if (deviceCount >= 2) {\n",
            "    cudaDeviceProp prop[64];\n",
            "    int gpuid[64];  // we want to find the first two GPUs that can support P2P\n",
            "    int gpu_p2p_count = 0;\n",
            "\n",
            "    for (int i = 0; i < deviceCount; i++) {\n",
            "      checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));\n",
            "\n",
            "      // Only boards based on Fermi or later can support P2P\n",
            "      if ((prop[i].major >= 2)\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "          // on Windows (64-bit), the Tesla Compute Cluster driver for windows\n",
            "          // must be enabled to support this\n",
            "          && prop[i].tccDriver\n",
            "#endif\n",
            "      ) {\n",
            "        // This is an array of P2P capable GPUs\n",
            "        gpuid[gpu_p2p_count++] = i;\n",
            "      }\n",
            "    }\n",
            "\n",
            "    // Show all the combinations of support P2P GPUs\n",
            "    int can_access_peer;\n",
            "\n",
            "    if (gpu_p2p_count >= 2) {\n",
            "      for (int i = 0; i < gpu_p2p_count; i++) {\n",
            "        for (int j = 0; j < gpu_p2p_count; j++) {\n",
            "          if (gpuid[i] == gpuid[j]) {\n",
            "            continue;\n",
            "          }\n",
            "          checkCudaErrors(\n",
            "              cudaDeviceCanAccessPeer(&can_access_peer, gpuid[i], gpuid[j]));\n",
            "          printf(\"> Peer access from %s (GPU%d) -> %s (GPU%d) : %s\\n\",\n",
            "                 prop[gpuid[i]].name, gpuid[i], prop[gpuid[j]].name, gpuid[j],\n",
            "                 can_access_peer ? \"Yes\" : \"No\");\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "\n",
            "  // csv masterlog info\n",
            "  // *****************************\n",
            "  // exe and CUDA driver name\n",
            "  printf(\"\\n\");\n",
            "  std::string sProfileString = \"deviceQuery, CUDA Driver = CUDART\";\n",
            "  char cTemp[16];\n",
            "\n",
            "  // driver version\n",
            "  sProfileString += \", CUDA Driver Version = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d.%d\", driverVersion/1000, (driverVersion%100)/10);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", driverVersion / 1000,\n",
            "           (driverVersion % 100) / 10);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "\n",
            "  // Runtime version\n",
            "  sProfileString += \", CUDA Runtime Version = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d.%d\", runtimeVersion/1000, (runtimeVersion%100)/10);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", runtimeVersion / 1000,\n",
            "           (runtimeVersion % 100) / 10);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "\n",
            "  // Device count\n",
            "  sProfileString += \", NumDevs = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d\", deviceCount);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d\", deviceCount);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "  sProfileString += \"\\n\";\n",
            "  printf(\"%s\", sProfileString.c_str());\n",
            "\n",
            "  printf(\"Result = PASS\\n\");\n",
            "\n",
            "  // finish\n",
            "  exit(EXIT_SUCCESS);\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYrd0V6hJbOq",
        "outputId": "44fc36ab-1687-45a1-a624-6d54b5f11baf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-ktok621s\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-ktok621s\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=4cf5bf958ae4b7aa8e4001eeeeda8917200a9ef5e6024466e3d446c8b40e3044\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wfbj_1xf/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery/src\n",
            "Out bin /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIFpBiQuJp4z",
        "outputId": "9319920b-71d0-46ac-da36-cdee818e4d8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arb4GPV3JvpG",
        "outputId": "ee16ad94-1b1f-4a86-d43e-5ee5530053a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%cu\n",
        "/****** calculate pi *******/\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "// For the CUDA runtime routines (prefixed with \"cuda_\")\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include <memory>\n",
        "#include <iostream>\n",
        "#include <sys/time.h>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// CUDA-C includes\n",
        "#include <cuda.h>\n",
        "\n",
        "int BLOCKSPERGRID  = 40;\n",
        "int NUMTHREADS = 10240; // (40 Multiprocessors * 64 CUDA Cores/MP) * 4 \n",
        "#define ITERATIONS 2e09\n",
        "\n",
        "/*****************************************************************************\n",
        "/*kernel\n",
        "*****************************************************************************/\n",
        "\n",
        "\n",
        "__global__ void calculatePi(double *piTotal, long int iterations, int totalThreads)\n",
        "{   long int initIteration, endIteration;\n",
        "    long int i = 0;\n",
        "    double piPartial;\n",
        "    \n",
        "    int index = (blockDim.x * blockIdx.x) + threadIdx.x;\n",
        "\n",
        "    initIteration = (iterations/totalThreads) * index;\n",
        "    endIteration = initIteration + (iterations/totalThreads) - 1;\n",
        "    \n",
        "    i = initIteration;\n",
        "    piPartial = 0;\n",
        "    \n",
        "    do{\n",
        "        piPartial = piPartial + (double)(4.0 / ((i*2)+1));\n",
        "        i++;\n",
        "        piPartial = piPartial - (double)(4.0 / ((i*2)+1));\n",
        "        i++;\n",
        "    }while(i < endIteration);\n",
        "\n",
        "    piTotal[index] = piPartial;\n",
        "    \n",
        "    __syncthreads();\n",
        "    if(index == 0){\n",
        "        for(i = 1; i < totalThreads; i++)\n",
        "            piTotal[0] = piTotal[0] + piTotal[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "/******************************************************************************/\n",
        "\n",
        "\n",
        "int main()\n",
        "{   \n",
        "    cudaSetDevice(0);\n",
        "    cudaDeviceProp deviceProp;\n",
        "    cudaGetDeviceProperties(&deviceProp, 0);\n",
        " \n",
        "    struct timeval tval_before, tval_after, tval_result;\n",
        "\n",
        "    gettimeofday(&tval_before, NULL);\n",
        " \n",
        "\n",
        "    int blocksPerGrid, threadsPerBlock, i, size;\n",
        "    long int iterations;\n",
        "    int totalThreads;\n",
        "    double *h_pitotal, *d_pitotal;\n",
        "    \n",
        "    blocksPerGrid = BLOCKSPERGRID;\n",
        "    cudaError_t err = cudaSuccess;\n",
        "\n",
        "    size = sizeof(double)*NUMTHREADS;\n",
        "    h_pitotal = (double *)malloc(size);\n",
        "    if ( h_pitotal == NULL){\n",
        "        fprintf(stderr, \"Failed to allocate host vectors!\\n\");\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "    \n",
        "    for(i = 0; i < NUMTHREADS; i++)\n",
        "        h_pitotal[i] = 0.0;\n",
        "\n",
        "    err = cudaMalloc((void **)&d_pitotal, size);\n",
        "    if (err != cudaSuccess){\n",
        "        fprintf(stderr, \"Failed to allocate device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "    \n",
        "    err = cudaMemcpy(d_pitotal, h_pitotal, sizeof(double)*NUMTHREADS, cudaMemcpyHostToDevice);\n",
        "    if (err != cudaSuccess){\n",
        "        fprintf(stderr, \"Failed to copy vector C from device to host (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Lanzar KERNEL\n",
        "    threadsPerBlock = NUMTHREADS/blocksPerGrid;\n",
        "    totalThreads = blocksPerGrid * threadsPerBlock;\n",
        "    iterations = ITERATIONS;\n",
        "    printf(\"CUDA kernel launch with %d blocks of %d threads Total: %i       \", blocksPerGrid, threadsPerBlock, totalThreads  );\n",
        "    calculatePi<<<blocksPerGrid, threadsPerBlock>>>(d_pitotal, iterations, totalThreads);\n",
        "    err = cudaGetLastError();\n",
        "    if (err != cudaSuccess){\n",
        "        fprintf(stderr, \"Failed to launch vectorAdd kernel (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    err = cudaMemcpy(h_pitotal, d_pitotal, size, cudaMemcpyDeviceToHost);\n",
        "    if (err != cudaSuccess){\n",
        "        fprintf(stderr, \"Failed to copy vector C from device to host (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    err = cudaFree(d_pitotal);\n",
        "    if (err != cudaSuccess){\n",
        "        fprintf(stderr, \"Failed to free device vector C (error code %s)!\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    printf(\"Calculated pi: %.12f\", *h_pitotal);\n",
        "    // Free host memory\n",
        "\n",
        "    free(h_pitotal);\n",
        "    err = cudaDeviceReset();\n",
        "    if (err != cudaSuccess){\n",
        "        fprintf(stderr, \"Failed to deinitialize the device! error=%s\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        " \n",
        "    gettimeofday(&tval_after, NULL);\n",
        "\n",
        "    timersub(&tval_after, &tval_before, &tval_result);\n",
        "\n",
        "    printf(\"\\nTime elapsed: %ld.%06ld\\n\", (long int)tval_result.tv_sec, (long int)tval_result.tv_usec);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA kernel launch with 40 blocks of 256 threads Total: 10240       Calculated pi: 3.141592636327\n",
            "Time elapsed: 0.411239\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}